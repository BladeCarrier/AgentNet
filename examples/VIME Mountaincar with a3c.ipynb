{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whatami\n",
    "\n",
    "I am a simple experiment on using VIME with actor-critic agent setup for MountainCar problem.\n",
    "\n",
    "Vime performance varies greatly depending on BNN.curiosity parameter so pls pay attention to it :)\n",
    "* Large curiosity makes agent quickly learn to do weird things, some of which improve it's policy\n",
    "* Low curiosity is close to vanilla a2c\n",
    "\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "\n",
    "\n",
    "## New to Lasagne and AgentNet?\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-14 19:14:23--  https://raw.githubusercontent.com/justheuristic/vime/7e20be2231d680187dc79ad3e84b9658996d870c/bnn.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 64:ff9b::9765:2485\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|64:ff9b::9765:2485|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2747 (2.7K) [text/plain]\n",
      "Saving to: ‘bnn.py’\n",
      "\n",
      "100%[======================================>] 2,747       --.-K/s   in 0s      \n",
      "\n",
      "2017-01-14 19:14:23 (35.6 MB/s) - ‘bnn.py’ saved [2747/2747]\n",
      "\n",
      "--2017-01-14 19:14:24--  https://raw.githubusercontent.com/justheuristic/vime/7e20be2231d680187dc79ad3e84b9658996d870c/curiosity.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 64:ff9b::9765:2485\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|64:ff9b::9765:2485|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3223 (3.1K) [text/plain]\n",
      "Saving to: ‘curiosity.py’\n",
      "\n",
      "100%[======================================>] 3,223       --.-K/s   in 0s      \n",
      "\n",
      "2017-01-14 19:14:24 (41.2 MB/s) - ‘curiosity.py’ saved [3223/3223]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/justheuristic/vime/7e20be2231d680187dc79ad3e84b9658996d870c/bnn.py -O bnn.py\n",
    "!wget https://raw.githubusercontent.com/justheuristic/vime/7e20be2231d680187dc79ad3e84b9658996d870c/curiosity.py -O curiosity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"floatX=float32\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS=\"floatX=float32\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "GAME = \"MountainCar-v0\"\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "N_AGENTS = 1\n",
    "SEQ_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:14:25,967] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.52388428 -0.0010054 ]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(GAME)\n",
    "env.reset()\n",
    "\n",
    "obs = env.step(0)[0]\n",
    "\n",
    "action_names = np.array([\"left\",'stop',\"right\"]) #i guess so... i may be wrong\n",
    "state_size = len(obs)\n",
    "\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using shallow neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer,DenseLayer,NonlinearityLayer,batch_norm,dropout\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,state_size))\n",
    "\n",
    "dense0 = DenseLayer(observation_layer,100,name='dense1')\n",
    "dense1 = DenseLayer(dense0,256,name='dense2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues\n",
    "\n",
    "policy_layer = DenseLayer(dense1,\n",
    "                   num_units = env.action_space.n,\n",
    "                   nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                   name=\"q-evaluator layer\")\n",
    "\n",
    "\n",
    "V_layer = DenseLayer(dense1, 1, nonlinearity=None,name=\"state values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pick actions at random proportionally to te probabilities\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "action_layer = ProbabilisticResolver(policy_layer,\n",
    "                                     name=\"e-greedy action picker\",\n",
    "                                     assume_normalized=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(policy_layer,V_layer),\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dense1.W,\n",
       " dense1.b,\n",
       " dense2.W,\n",
       " dense2.b,\n",
       " q-evaluator layer.W,\n",
       " q-evaluator layer.b,\n",
       " state values.W,\n",
       " state values.b]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((action_layer,V_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:14:26,824] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(agent,GAME, N_AGENTS,max_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['right' 'left' 'left' 'right' 'left' 'left' 'left']]\n",
      "[[-1. -1. -1. -1. -1. -1.  0.]]\n",
      "CPU times: user 4.38 ms, sys: 145 µs, total: 4.52 ms\n",
      "Wall time: 4.41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_names[action_log])\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a2c loss\n",
    "\n",
    "Here we define obective function for actor-critic (one-step) RL.\n",
    "\n",
    "* We regularize policy with expected inverse action probabilities (discouraging very small probas) to make objective numerically stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100,replace=True)\n",
    "\n",
    "_,_,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import a2c\n",
    "\n",
    "\n",
    "#Train via actor-critic (see here - https://www.youtube.com/watch?v=KHZVXao4qXs)\n",
    "\n",
    "elwise_mse_loss = a2c.get_elementwise_objective(policy_seq,V_seq[:,:,0],\n",
    "                                                       replay.actions[0],\n",
    "                                                       replay.rewards,\n",
    "                                                       replay.is_alive,\n",
    "                                                       gamma_or_gammas=0.99)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "reg = T.mean((1./policy_seq).sum(axis=-1))\n",
    "loss += 0.001*reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.rmsprop(loss,weights,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:14:42,407] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:14:42,410] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:14:42,411] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-01-14 19:14:42,412] Starting new video recorder writing to /root/anet/agentnet/examples/records/openaigym.video.0.9649.video000000.mp4\n",
      "[2017-01-14 19:14:52,827] Tried to pass invalid video frame, marking as broken: Your frame has shape (1, 1, 3), but the VideoRecorder is configured for shape (400, 600, 3).\n",
      "[2017-01-14 19:14:52,900] Cleaning up paths for broken video recorder: path=/root/anet/agentnet/examples/records/openaigym.video.0.9649.video000000.mp4 metadata_path=/root/anet/agentnet/examples/records/openaigym.video.0.9649.video000000.meta.json\n",
      "[2017-01-14 19:14:52,912] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps with reward=-200.0\n"
     ]
    }
   ],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"<fill in url from above>\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_path=\"<fill in url from above>\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bnn import bbpwrap, NormalApproximation,sample_output\n",
    "from lasagne.layers import EmbeddingLayer\n",
    "import theano.tensor as T\n",
    "@bbpwrap(NormalApproximation())\n",
    "class BayesDenseLayer(DenseLayer):pass\n",
    "@bbpwrap(NormalApproximation())\n",
    "class BayesEmbLayer(EmbeddingLayer):pass\n",
    "\n",
    "from curiosity import compile_vime_reward\n",
    "\n",
    "class BNN:\n",
    "    curiosity=0.01\n",
    "    target_rho = 1\n",
    "    \n",
    "    l_state = InputLayer((None,state_size),name='state var')\n",
    "    l_action = InputLayer((None,),input_var=T.ivector())\n",
    "\n",
    "    l_action_emb = BayesEmbLayer(l_action,env.action_space.n, 3)    \n",
    "    \n",
    "    l_concat = lasagne.layers.concat([l_action_emb,l_state])\n",
    "    \n",
    "    l_dense = BayesDenseLayer(l_concat,num_units=50,\n",
    "                              nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    l_out = BayesDenseLayer(l_dense,num_units=state_size,\n",
    "                            nonlinearity=None)\n",
    "        \n",
    "    params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "    ###training###\n",
    "    pred_states = lasagne.layers.get_output(l_out)\n",
    "    next_states = T.matrix(\"next states\")\n",
    "    mse = lasagne.objectives.squared_error(pred_states,next_states).mean()\n",
    "    \n",
    "    #replace logposterior with simple regularization on rho cuz we're lazy\n",
    "    reg = sum([lasagne.objectives.squared_error(rho,target_rho).mean() \n",
    "              for rho in lasagne.layers.get_all_params(l_out,rho=True)])\n",
    "    \n",
    "    loss = mse+ 0.01*reg\n",
    "    \n",
    "    updates = lasagne.updates.adam(loss,params)\n",
    "    \n",
    "    train_step = theano.function([l_state.input_var,l_action.input_var,next_states],\n",
    "                                 loss,updates=updates)\n",
    "    \n",
    "    ###sample random sessions from pool###\n",
    "    observations, = replay.observations\n",
    "    actions, = replay.actions\n",
    "    observations_flat = observations[:,:-1].reshape((-1,)+tuple(observations.shape[2:]))\n",
    "    actions_flat = actions[:,:-1].reshape((-1,))\n",
    "    next_observations_flat = observations[:,1:].reshape((-1,)+tuple(observations.shape[2:]))\n",
    "    sample_from_pool = theano.function([],[observations_flat,actions_flat,next_observations_flat])\n",
    "\n",
    "    \n",
    "    ###curiosity reward### aka KL(qnew,qold)\n",
    "    get_vime_reward_elwise = compile_vime_reward(l_out,l_state,l_action,params,n_samples=10)\n",
    "    \n",
    "    vime_reward_ma = 10.\n",
    "    @staticmethod\n",
    "    def add_vime_reward(observations,actions,rewards,is_alive,h0):\n",
    "        assert isinstance(observations,np.ndarray)\n",
    "        observations_flat = observations[:,:-1].reshape((-1,)+observations.shape[2:]).astype('float32')\n",
    "        actions_flat = actions[:,:-1].reshape((-1,)).astype('int32')\n",
    "        next_observations_flat = observations[:,1:].reshape((-1,)+observations.shape[2:]).astype('float32')\n",
    "\n",
    "        vime_rewards = BNN.get_vime_reward_elwise(observations_flat,actions_flat,next_observations_flat)\n",
    "        vime_rewards = np.concatenate([vime_rewards.reshape(rewards[:,:-1].shape),\n",
    "                                       np.zeros_like(rewards[:,-1:]),], axis=1)\n",
    "        #normalize by moving average\n",
    "        BNN.vime_reward_ma = 0.99*BNN.vime_reward_ma + 0.01*vime_rewards.mean()\n",
    "        \n",
    "        surrogate_rewards = rewards + BNN.curiosity/BNN.vime_reward_ma*vime_rewards\n",
    "        return (observations,actions,surrogate_rewards,is_alive,h0)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 180.96it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 386.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#pre-fill pool\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    pool.update(SEQ_LENGTH,append=True,preprocess=BNN.add_vime_reward)\n",
    "\n",
    "#pre-train BNN (mitigate training lag on first iterations where BNN is stupid)\n",
    "for i in tqdm(range(1000)):\n",
    "    BNN.train_step(*BNN.sample_from_pool())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/10000 [00:17<27:47,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=100\treward/step=-0.98987\tpool_size=2001\tvime ma=1.73620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/10000 [00:34<28:22,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=200\treward/step=-0.98970\tpool_size=3001\tvime ma=1.17607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [00:51<27:04,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=300\treward/step=-0.98962\tpool_size=4001\tvime ma=0.77445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [01:08<27:20,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=400\treward/step=-0.98957\tpool_size=5001\tvime ma=0.49411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 499/10000 [01:25<27:28,  5.76it/s][2017-01-14 19:16:33,792] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:16:33,797] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:16:33,798] Clearing 3 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=500\treward/step=-0.98952\tpool_size=6001\tvime ma=0.36471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:16:34,171] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      "  5%|▌         | 501/10000 [01:26<39:51,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 601/10000 [01:43<27:36,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=600\treward/step=-0.98948\tpool_size=7001\tvime ma=0.25751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 701/10000 [02:00<26:42,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=700\treward/step=-0.98945\tpool_size=8001\tvime ma=0.19955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 801/10000 [02:18<27:25,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=800\treward/step=-0.98943\tpool_size=9001\tvime ma=0.13555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 901/10000 [02:36<27:18,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=900\treward/step=-0.98941\tpool_size=10000\tvime ma=0.09811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 999/10000 [02:54<26:24,  5.68it/s][2017-01-14 19:18:02,474] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:18:02,478] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:18:02,479] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1000\treward/step=-0.98931\tpool_size=10000\tvime ma=0.08020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:18:02,859] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 10%|█         | 1001/10000 [02:54<38:21,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1101/10000 [03:12<26:09,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1100\treward/step=-0.98926\tpool_size=10000\tvime ma=0.06525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1201/10000 [03:30<25:34,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1200\treward/step=-0.98925\tpool_size=10000\tvime ma=0.04462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1301/10000 [03:48<25:23,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1300\treward/step=-0.98921\tpool_size=10000\tvime ma=0.03692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1401/10000 [04:05<25:32,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1400\treward/step=-0.98917\tpool_size=10000\tvime ma=0.03049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1499/10000 [04:22<24:49,  5.71it/s][2017-01-14 19:19:31,273] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:19:31,276] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:19:31,277] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1500\treward/step=-0.98913\tpool_size=10000\tvime ma=0.02663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:19:31,677] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 15%|█▌        | 1501/10000 [04:23<36:59,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1601/10000 [04:41<24:36,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1600\treward/step=-0.98909\tpool_size=10000\tvime ma=0.02198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1701/10000 [04:59<23:49,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1700\treward/step=-0.98903\tpool_size=10000\tvime ma=0.01954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1801/10000 [05:16<23:16,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1800\treward/step=-0.98885\tpool_size=10000\tvime ma=0.01844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1901/10000 [05:34<23:39,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1900\treward/step=-0.98852\tpool_size=10000\tvime ma=0.01846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1999/10000 [05:50<23:08,  5.76it/s][2017-01-14 19:20:59,260] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:20:59,263] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:20:59,264] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2000\treward/step=-0.98819\tpool_size=10000\tvime ma=0.01844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:20:59,576] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 20%|██        | 2001/10000 [05:51<32:26,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -152.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2101/10000 [06:09<23:19,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2100\treward/step=-0.98783\tpool_size=10000\tvime ma=0.01813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2201/10000 [06:27<23:16,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2200\treward/step=-0.98744\tpool_size=10000\tvime ma=0.01836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2301/10000 [06:45<22:54,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2300\treward/step=-0.98707\tpool_size=10000\tvime ma=0.01929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2401/10000 [07:03<22:15,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2400\treward/step=-0.98670\tpool_size=10000\tvime ma=0.02002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2499/10000 [07:20<22:20,  5.60it/s][2017-01-14 19:22:28,940] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:22:28,942] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:22:28,944] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2500\treward/step=-0.98636\tpool_size=10000\tvime ma=0.01988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:22:29,203] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 25%|██▌       | 2501/10000 [07:21<29:04,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -131.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2601/10000 [07:39<21:42,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2600\treward/step=-0.98603\tpool_size=10000\tvime ma=0.02039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2701/10000 [07:56<21:26,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2700\treward/step=-0.98570\tpool_size=10000\tvime ma=0.02545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2801/10000 [08:15<21:08,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2800\treward/step=-0.98550\tpool_size=10000\tvime ma=0.02543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2901/10000 [08:33<21:38,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2900\treward/step=-0.98543\tpool_size=10000\tvime ma=0.02535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2999/10000 [08:51<21:34,  5.41it/s][2017-01-14 19:23:59,802] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:23:59,805] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:23:59,806] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3000\treward/step=-0.98535\tpool_size=10000\tvime ma=0.02871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:24:00,026] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 30%|███       | 3001/10000 [08:52<26:30,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -117.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3101/10000 [09:09<20:34,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3100\treward/step=-0.98531\tpool_size=10000\tvime ma=0.03177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3201/10000 [09:28<21:24,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3200\treward/step=-0.98537\tpool_size=10000\tvime ma=0.03298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3301/10000 [09:46<20:23,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3300\treward/step=-0.98539\tpool_size=10000\tvime ma=0.03087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3401/10000 [10:05<20:59,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3400\treward/step=-0.98539\tpool_size=10000\tvime ma=0.03484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3499/10000 [10:23<19:56,  5.43it/s][2017-01-14 19:25:31,533] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:25:31,536] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:25:31,537] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3500\treward/step=-0.98535\tpool_size=10000\tvime ma=0.03731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:25:31,765] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 35%|███▌      | 3500/10000 [10:23<27:35,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -111.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3601/10000 [10:42<19:38,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3600\treward/step=-0.98530\tpool_size=10000\tvime ma=0.03967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3701/10000 [11:00<20:05,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3700\treward/step=-0.98525\tpool_size=10000\tvime ma=0.03859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3801/10000 [11:18<18:13,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3800\treward/step=-0.98517\tpool_size=10000\tvime ma=0.04288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3901/10000 [11:37<18:21,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3900\treward/step=-0.98513\tpool_size=10000\tvime ma=0.04379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3999/10000 [11:55<18:22,  5.44it/s][2017-01-14 19:27:03,463] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:27:03,465] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:27:03,466] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4000\treward/step=-0.98510\tpool_size=10000\tvime ma=0.04581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:27:03,699] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 40%|████      | 4001/10000 [11:55<23:37,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -113.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4101/10000 [12:14<17:45,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4100\treward/step=-0.98508\tpool_size=10000\tvime ma=0.04525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4201/10000 [12:33<17:37,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4200\treward/step=-0.98499\tpool_size=10000\tvime ma=0.04779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4301/10000 [12:51<17:39,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4300\treward/step=-0.98490\tpool_size=10000\tvime ma=0.05222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4400/10000 [13:09<16:55,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4400\treward/step=-0.98486\tpool_size=10000\tvime ma=0.05298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4499/10000 [13:27<16:50,  5.45it/s][2017-01-14 19:28:36,215] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:28:36,281] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:28:36,287] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4500\treward/step=-0.98484\tpool_size=10000\tvime ma=0.05615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:28:36,533] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 45%|████▌     | 4500/10000 [13:28<25:48,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -112.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4601/10000 [13:46<16:05,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4600\treward/step=-0.98482\tpool_size=10000\tvime ma=0.05942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4701/10000 [14:05<17:34,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4700\treward/step=-0.98484\tpool_size=10000\tvime ma=0.05844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4801/10000 [14:24<16:18,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4800\treward/step=-0.98485\tpool_size=10000\tvime ma=0.06298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4901/10000 [14:43<15:41,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4900\treward/step=-0.98486\tpool_size=10000\tvime ma=0.05928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 4999/10000 [15:01<15:17,  5.45it/s][2017-01-14 19:30:10,140] Making new env: MountainCar-v0\n",
      "[2017-01-14 19:30:10,144] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-14 19:30:10,146] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5000\treward/step=-0.98485\tpool_size=10000\tvime ma=0.06338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-14 19:30:10,366] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/agentnet/examples/records')\n",
      " 50%|█████     | 5001/10000 [15:02<19:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -115.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5101/10000 [15:21<14:58,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5100\treward/step=-0.98483\tpool_size=10000\tvime ma=0.06329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5200/10000 [15:41<20:47,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5200\treward/step=-0.98482\tpool_size=10000\tvime ma=0.06664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5300/10000 [16:07<21:48,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5300\treward/step=-0.98483\tpool_size=10000\tvime ma=0.06879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5400/10000 [16:33<19:44,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5400\treward/step=-0.98482\tpool_size=10000\tvime ma=0.07210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 5491/10000 [16:56<19:17,  3.89it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "loss = 0\n",
    "for i in tqdm(range(10000)):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    for i in range(10):\n",
    "        pool.update(SEQ_LENGTH,append=True,preprocess=BNN.add_vime_reward)\n",
    "\n",
    "    for i in range(10):\n",
    "        loss = loss*0.99 + train_step()*0.01\n",
    "    \n",
    "    for i in range(10):\n",
    "        BNN.train_step(*BNN.sample_from_pool())\n",
    "\n",
    "    \n",
    "    if epoch_counter%100==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = np.average(pool.experience_replay.rewards.get_value()[:,:-1],\n",
    "                                      weights=1+pool.experience_replay.is_alive.get_value()[:,:-1])\n",
    "        pool_size = pool.experience_replay.rewards.get_value().shape[0]\n",
    "        print(\"iter=%i\\treward/step=%.5f\\tpool_size=%i\\tvime ma=%.5f\"%(epoch_counter,\n",
    "                                                         pool_mean_reward,\n",
    "                                                         pool_size,\n",
    "                                                         BNN.vime_reward_ma))\n",
    "        \n",
    "\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        n_games = 10\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iters,map(np.mean,session_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "_,_,_,_,(pool_policy,pool_V) = agent.get_sessions(\n",
    "    pool.experience_replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,)\n",
    "\n",
    "plt.scatter(\n",
    "    *pool.experience_replay.observations[0].get_value().reshape([-1,2]).T,\n",
    "    c = pool_V.ravel().eval(),\n",
    "    alpha = 0.1)\n",
    "plt.title(\"predicted state values\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs_x,obs_y = pool.experience_replay.observations[0].get_value().reshape([-1,2]).T\n",
    "optimal_actid = pool_policy.argmax(-1).ravel().eval()\n",
    "\n",
    "for i in range(3):\n",
    "    sel = optimal_actid==i\n",
    "    plt.scatter(obs_x[sel],obs_y[sel],\n",
    "                c=['red','blue','green'][i],\n",
    "                alpha = 0.1,label=action_names[i])\n",
    "    \n",
    "plt.title(\"most likely action id\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
